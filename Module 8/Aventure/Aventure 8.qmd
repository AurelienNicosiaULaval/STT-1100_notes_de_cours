---
title: "Aventure 8 - DonnÃ©es ouvertes du QuÃ©bec"
subtitle: "STT-1100 Introduction Ã  la science des donnÃ©es"
author:
  - institute: "UniversitÃ© Laval"
editor: visual
editor_options:
  chunk_output_type: console
format:
  html:
    toc: true
    toc-title: Plan
    css: [../../css/base_css.css]
    self-contained: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(purrr)
```

# ğŸ¬ Mise en situation : Plongez dans la peau dâ€™unÂ·e consultantÂ·e freelance

Vous travaillez comme **freelancer** en science des donnÃ©es. Ã€ la diffÃ©rence dâ€™un poste salariÃ©, vous Ãªtes mandatÃ©Â·e ponctuellement par des clientÂ·eÂ·s pour rÃ©soudre des problÃ¨mes spÃ©cifiques. Vous devez Ãªtre autonome, rigoureuxÂ·se et capable de livrer des solutions efficaces, rÃ©utilisables et bien documentÃ©es.

Les consultantÂ·eÂ·s freelance sont souvent appelÃ©s pour leur capacitÃ© Ã  dÃ©bloquer des projets rapidement et efficacement. Ils livrent des outils bien documentÃ©s, testables, et faciles Ã  maintenir par leurs clientÂ·eÂ·s. La clartÃ© du code, le respect des standards et lâ€™Ã©thique sont donc des Ã©lÃ©ments clÃ©s de leur pratique.

Aujourdâ€™hui, vous Ãªtes engagÃ©Â·e par **Marie-Pier**, directrice de recherche Ã  lâ€™Institut quÃ©bÃ©cois pour les donnÃ©es durables. Elle souhaite explorer le portail [donnÃ©esquebec.ca](https://www.donneesquebec.ca) pour identifier les jeux de donnÃ©es les plus rÃ©cents et pertinents pour ses projets. Votre mission : **concevoir un outil de scraping fiable** pour extraire les titres, producteurs et catÃ©gories de jeux de donnÃ©es, tout en respectant les bonnes pratiques Ã©thiques.

::: callout-note
**Votre interlocutrice : Marie-Pier** \
Elle vous accompagne tout au long de cette aventure : elle posera des questions clÃ©s, vÃ©rifiera vos rÃ©sultats, et vous aidera Ã  affiner vos livrables.
:::

## Objectifs de l'aventure

- Comprendre les bases du web scraping avec `rvest`.
- CrÃ©er une fonction pour extraire des mÃ©tadonnÃ©es depuis une page web.
- Automatiser l'extraction sur plusieurs pages.
- Explorer les tendances dans les donnÃ©es ouvertes du QuÃ©bec.
- Ã‰valuer la faisabilitÃ© Ã©thique et technique du scraping sur d'autres sites web.

---

# ğŸ“š PrÃ©requis

::: {.callout-important title="ğŸ§‘â€ğŸ’¼ - Question de Marie-Pier"}
*Â« Que peut-on lÃ©galement extraire de ce portail ? Le scraping est-il autorisÃ© ici ? Â»*
:::

Pour rÃ©pondre Ã  cette question, nous allons dâ€™abord consulter le fichier `robots.txt` du site. Ce fichier indique les rÃ¨gles que les robots dâ€™indexation (et donc de scraping) doivent suivre.

```{r exploration-ethique}
# Affichez le robots.txt
robots <- readLines("https://www.donneesquebec.ca/robots.txt")
cat("\nContenu du fichier robots.txt :\n")
writeLines(robots)

# Analyse rapide
disinstructions <- robots[grepl("^Disallow", robots)]
cat("\n\nChemins interdits aux robots :\n")
writeLines(disinstructions)

# Note Ã©thique
cat("\n\n\u2139\ufe0f Note :\nLes jeux de donnÃ©es publics listÃ©s dans les pages de rÃ©sultat ne sont pas explicitement restreints.\nLe scraping des pages principales de recherche est donc permis, tant qu'on Ã©vite les chemins /api/, /dataset/rate/, etc.\n")

```

# ğŸ“š Comprendre le web scraping avec `rvest`

Dans cette section, vous dÃ©couvrirez les fonctions essentielles du package `rvest`. Votre objectif : apprendre Ã  extraire du contenu HTML structurÃ© depuis un site web.

```{r lecture-html}
url <- "https://www.donneesquebec.ca/recherche/?sort=metadata_modified+desc&page=1"
page <- read_html(url)
page
```

La fonction `read_html()` tÃ©lÃ©charge et convertit la page web pour en permettre la manipulation.

Marie-Pier vous demande :

::: {.callout-important title="ğŸ§‘â€ğŸ’¼ - Question de Marie-Pier"}
*Â« Peux-tu me montrer Ã  quoi ressemble la structure de cette page ? Est-ce que tu peux repÃ©rer un Ã©lÃ©ment intÃ©ressant ? Â»*
:::

On peut maintenant cibler les Ã©lÃ©ments HTML avec `html_nodes()` (ou `html_elements()` dans les versions rÃ©centes) :

```{r test-nodes}
blocs <- html_nodes(page, ".dataset-content")
length(blocs)
blocs[[1]]
```

Pour extraire du texte dâ€™un nÅ“ud HTML :

```{r test-text}
html_text(blocs[[1]])
```

Maintenant, testons lâ€™extraction du **titre** :

```{r test-titre}
html_nodes(blocs[[1]], ".dataset-heading a") %>% html_text(trim = TRUE)
```

Et pour les **producteurs** ? Il faut repÃ©rer une sous-structure contenant lâ€™information :

```{r test-producteur}
orgs <- html_nodes(blocs[[1]], ".dqc-org-cat") %>% html_text(trim = TRUE)
orgs
```

On peut filtrer le bon Ã©lÃ©ment avec `grepl()` puis nettoyer la chaÃ®ne avec `gsub()` :

```{r extract-producer}
org <- orgs[grepl("^Organisation", orgs)][1]
org_clean <- gsub("^Organisation : ", "", org)
org_clean
```

::: callout-tip
ğŸ’¡ **Pourquoi utiliser `map_chr()` ?** \
`map_chr()` appartient au package **purrr**, qui fait partie du `tidyverse`. Cette fonction permet dâ€™appliquer une fonction Ã  chaque Ã©lÃ©ment dâ€™une liste (ici chaque bloc HTML), et de retourner un vecteur de caractÃ¨res. Câ€™est parfait lorsquâ€™on veut une valeur texte par bloc.

> Exemple :
> ```r
> producteurs <- map_chr(blocs, function(bloc) {
>   orgs <- html_nodes(bloc, ".dqc-org-cat") %>% html_text(trim = TRUE)
>   org <- orgs[grepl("^Organisation", orgs)][1]
>   gsub("^Organisation : ", "", org)
> })
> ```
:::

::: {.callout-tip}
ğŸ’¡ **Ã€ vous de jouer** : Ã€ partir de ce mÃªme bloc HTML, trouvez comment extraire :
- les **catÃ©gories** associÃ©es Ã  chaque jeu de donnÃ©es
:::




---

# âœï¸ CrÃ©ation guidÃ©e de la fonction `scrape_page()`

Marie-Pier souhaite que vous crÃ©iez une fonction rÃ©utilisable nommÃ©e `scrape_page()` qui prend en argument une URL et retourne un `data.frame` avec les colonnes : `titre`, `producteur`, `categorie`.

Voici un squelette Ã  complÃ©ter :

```{r, eval=FALSE}
scrape_page <- function(url) {
  # Lire le contenu HTML de la page
  page <- read_html(url)

  # SÃ©lectionner les blocs de rÃ©sultats individuels
  blocs <- html_nodes(page, ".dataset-content")

  # Titre (dÃ©jÃ  fait pour vous)
  titres <- html_nodes(blocs, ".dataset-heading a") %>% html_text(trim = TRUE)

  # Producteur (Ã  complÃ©ter)
  producteurs <- map_chr(blocs, function(bloc) {
    # ... votre code ici ...
  })

  # CatÃ©gorie (Ã  complÃ©ter)
  categories <- map_chr(blocs, function(bloc) {
    # ... votre code ici ...
  })

  # CrÃ©er un data.frame standard
  data.frame(
    titre = titres,
    producteur = producteurs,
    categorie = categories,
    stringsAsFactors = FALSE
  )
}
```

::: callout-note
ğŸ’¡ **Testez votre fonction avec la page 3** : elle devrait retourner les 20 jeux de donnÃ©es de cette page.
:::

---

# ğŸ” RÃ©pÃ©tition manuelle, puis boucle `for`

Testez maintenant l'extraction des **5 premiÃ¨res pages** en appelant plusieurs fois votre fonction :

```{r, eval=FALSE}
# Exemple manuel (Ã  complÃ©ter)
p1 <- scrape_page("...")
p2 <- scrape_page("...")
# etc.
```

::: {.callout-tip}
ğŸ’¬ **Marie-Pier** : Â« Est-ce que tu as remarquÃ© ce qui change dans lâ€™URL Ã  chaque fois ? Peux-tu gÃ©nÃ©raliser ce comportement ? Â»
:::

Rappel du **module 1** : une boucle permet d'automatiser un comportement rÃ©pÃ©titif. Voici un dÃ©but de boucle `for` Ã  complÃ©ter :

```{r, eval=FALSE}
resultats <- data.frame()

for (i in 1:5) {
  # Construire l'URL ici
  url <- "..."

  cat("Page", i, "en cours...\n")
  page_data <- scrape_page(url)

  resultats <- bind_rows(resultats, page_data)
  Sys.sleep(1)
}
```

---

# ğŸ” Exploration guidÃ©e par Marie-Pier

Utilisez le tableau `resultats` pour rÃ©pondre aux questions de votre cliente. Elle attend de vous des rÃ©sultats prÃ©cis, illustrÃ©s si nÃ©cessaire.

::: {.callout-important title="ğŸ§‘â€ğŸ’¼ - Questions de Marie-Pier"}
1. Quelles sont les catÃ©gories de jeux de donnÃ©es les plus frÃ©quentes ?
2. Quels organismes publient le plus ?
3. Observe-t-on une diversitÃ© de domaines ou une concentration sur quelques thÃ¨mes ?
:::

---

# ğŸ¤” RÃ©flexion Ã©thique

::: {.callout-note title="ğŸ’¡ Pour votre rapport"}
Rendez compte briÃ¨vement :

- Le site DonnÃ©es QuÃ©bec permet-il explicitement le scraping ?
- Quels comportements avez-vous adoptÃ©s pour rester respectueux ?
- Choisissez **deux autres sites web** (ex: bonjourquebec.com, ulaval.ca) et vÃ©rifiez si le scraping semble autorisÃ© ou non. Appuyez-vous sur les fichiers `robots.txt` ou les conditions d'utilisation.
:::

---

# ğŸ§ª DÃ©fi Ã  remettre

Vous devez remettre un **fichier `IDUL.R`** contenant votre fonction `scrape_page()`.

- Ce fichier doit Ãªtre placÃ© dans votre dÃ©pÃ´t GitHub Ã  l'endroit indiquÃ©.
- Nous testerons automatiquement votre fonction avec plusieurs pages.

ğŸ¯ Bonne chance â€” soyez rigoureux dans la conception de votre fonction !

---

# âœ… Conclusion de lâ€™aventure

Vous avez conÃ§u un outil de scraping fonctionnel et automatisÃ©, utilisÃ© une boucle `for`, extrait des mÃ©tadonnÃ©es structurÃ©es, et approfondi votre comprÃ©hension de lâ€™Ã©thique du scraping.

Bravo, consultantÂ·e ! ğŸ§ ğŸ’»
