---
title: "Exploration : Spectres de matrices de poids dans les rÃ©seaux de neurones"
subtitle: "Projet guidÃ© en apprentissage automatique et matrices alÃ©atoires"
author: "EncadrÃ© par AurÃ©lien Nicosia"
format:
  html:
    embed-resources: true
    toc: true
    toc-title: "Plan du projet"
    code-fold: true
    number-sections: true
---

# Objectif du projet

Ce projet vise Ã  faire dÃ©couvrir **comment la thÃ©orie des matrices alÃ©atoires (RMT)** peut Ã©clairer le comportement des **rÃ©seaux de neurones profonds**, en analysant les **spectres (valeurs singuliÃ¨res)** des **matrices de poids**.

Tu entraÃ®neras un **rÃ©seau de neurones simple (MLP)** sur un jeu de donnÃ©es comme **MNIST**, et tu analyseras comment les **valeurs singuliÃ¨res des poids** Ã©voluent au cours de l'entraÃ®nement, en les comparant Ã  la **loi de Marchenkoâ€“Pastur**.

Le but n'est pas la performance, mais la **comprÃ©hension mathÃ©matique** de ce qui se passe Ã  l'intÃ©rieur du rÃ©seau.

# Intuition du sujet

Un rÃ©seau de neurones est constituÃ© de **couches** qui rÃ©alisent des transformations linÃ©aires :

$$
\mathbf{y} = W \mathbf{x} + \mathbf{b}
$$

Chaque couche possÃ¨de donc une **matrice de poids** $W \in \mathbb{R}^{n \times p}$. Lorsqu'on initialise ces matrices avec des coefficients **alÃ©atoires**, elles ressemblent Ã  des **matrices alÃ©atoires classiques** : entrÃ©es i.i.d., centrÃ©es, variance contrÃ´lÃ©e.

La **loi de Marchenkoâ€“Pastur** dÃ©crit le **comportement asymptotique** du spectre (valeurs propres) de telles matrices alÃ©atoires. Elle fournit un cadre thÃ©orique pour **distinguer le bruit du signal** :

- si les valeurs propres/singuliÃ¨res de $W$ restent dans l'intervalle $[\lambda_-, \lambda_+]$, on est dans le bruit,
- si elles s'Ã©cartent de cet intervalle, on observe un **signal**.
- des **valeurs isolÃ©es** (outliers) signalent de la **structure apprise**.

# Exemple d'analyse

Tu vas :

1. **EntraÃ®ner un MLP** sur MNIST avec 3â€“4 couches.
2. **Sauvegarder les matrices de poids** de chaque couche Ã  diffÃ©rentes Ã©poques.
3. **Calculer leurs valeurs singuliÃ¨res** $\sigma_1, \ldots, \sigma_p$.
4. **Tracer l'histogramme** de ces valeurs (Empirical Spectral Density).
5. **Comparer avec la densitÃ© de Marchenkoâ€“Pastur** :
   - en fonction du ratio $q = p/n$,
   - en estimant la variance $\sigma^2$.

Tu observeras comment les spectres Ã©voluent avec lâ€™apprentissage. Lâ€™apparition dâ€™**outliers** dans les valeurs singuliÃ¨res est interprÃ©tÃ©e comme une manifestation de **structure utile apprise par le rÃ©seau**.

# Pistes de questions Ã  explorer

- Le spectre des poids Ã  lâ€™initialisation suit-il bien la loi de Marchenkoâ€“Pastur ?
- Quelle est la premiÃ¨re couche Ã  produire des outliers pendant l'entraÃ®nement ?
- Peut-on relier le **nombre ou l'Ã©nergie des outliers** Ã  la prÃ©cision du modÃ¨le ?
- L'utilisation d'une rÃ©gularisation (Dropout, L2) modifie-t-elle le spectre ?

# Lectures recommandÃ©es

Voici quelques lectures fondamentales pour bien comprendre les bases mathÃ©matiques du projet :

## ThÃ©orie des matrices alÃ©atoires

- Tao, T. (2012). *Topics in Random Matrix Theory*, AMS.  
  ğŸ“Œ Lire le **chapitre 4** sur la loi de Marchenkoâ€“Pastur.  
  ğŸ“ [https://terrytao.files.wordpress.com/2011/02/matrix-book.pdf](https://terrytao.files.wordpress.com/2011/02/matrix-book.pdf)

- Couillet, R. & Liao, Z. *Random Matrix Theory for Machine Learning* (2021).  
  ğŸ“Œ Lire les **sections 1 et 3** pour lâ€™intuition.  
  ğŸ“ [https://polaris.imag.fr/romain.couillet/docs/RMT_ML_Book.pdf](https://polaris.imag.fr/romain.couillet/docs/RMT_ML_Book.pdf)

## PCA et applications statistiques

- Johnstone, I. (2001). *On the distribution of the largest eigenvalue in PCA*.  
  Pour comprendre le lien entre **PCA, RMT et loi de Tracyâ€“Widom**.

# Ressources techniques

- Jeu de donnÃ©es : `torchvision.datasets.MNIST`
- RÃ©seau : MLP 3 ou 4 couches avec ReLU, SGD ou Adam
- Librairies : `torch`, `matplotlib`, `numpy`, `scipy`
- Code pour tracer la densitÃ© de Marchenkoâ€“Pastur fourni dans le notebook associÃ©

# Livrables

- ğŸ““ Un **notebook documentÃ©** montrant les Ã©tapes de l'analyse
- ğŸ“ˆ Des **figures claires** du spectre (avant / pendant / aprÃ¨s entraÃ®nement)
- ğŸ“„ Un **court rapport rÃ©flexif** en format Quarto (5â€“8 pages)

# Ã€ toi de jouer !

Tu peux faire Ã©voluer le projet selon ton intÃ©rÃªt : ajouter d'autres architectures, tester diffÃ©rentes initialisations, ou mÃªme comparer avec un modÃ¨le ayant des Ã©tiquettes alÃ©atoires. L'important, c'est de **mener une exploration rigoureuse et curieuse**, en utilisant la thÃ©orie pour interprÃ©ter ce que tu observes dans les matrices du rÃ©seau.

# Pour aller plus loin:

- [WeightWatcher](https://github.com/CalculatedContent/WeightWatcher) 
est un outil Python qui permet d'analyser les poids des rÃ©seaux de neurones et de calculer leurs valeurs singuliÃ¨res. Il est particuliÃ¨rement utile pour explorer la structure des poids et leur Ã©volution pendant l'entraÃ®nement.

- [Random Matrix Analysis of Neural Network Weight Matrices (Thamm et al., 2024)](https://openreview.net/pdf?id=41kpc2Nzwc) est un article qui explore l'utilisation de la thÃ©orie des matrices alÃ©atoires pour analyser les poids des rÃ©seaux de neurones. Il fournit des rÃ©sultats thÃ©oriques et empiriques sur la distribution des valeurs propres et leur relation avec la performance du modÃ¨le.
